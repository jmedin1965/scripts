#!/bin/bash

# extended glob pattern matching, uning this for regex matching
# in switch statement
shopt -s extglob

local_node="$(/bin/basename "$(/bin/readlink  /etc/pve/local)" )" # name of this node
declare -A cluster_pool=()  # list of pool names to location mappings
declare -A vm_sync_status=() # a list of vms and disks being processed, containing sync status
declare -A vm_node=()       # node vm is on
declare -A vm_type=()       # vm type, lxc or qemu
declare -A vm_status=()     # status, running stopped, etc
declare -A vm_local=()      # is it local to this node ? 1/0
declare -A vm_conf=()       # path to vm conf file

vm_storage_list=()             # a list of found disks, returned by get_vm_storage
declare -A vm_storage_from=()  # storage we are copying from, indexed by disk name
declare -A vm_storage_to=()    # storage we are copying to, indexed by disk name
declare -A vm_storage_disk=()  # the vm a disk belongs to, indexed by disk name

error_list=()               # a list of errors

# command line options
opt_force=""
opt_move=""
opt_dry_run=""

#syncoid options
opts=(
    "--preserve-properties"
)

main()
{
    prog="$(/usr/bin/basename "$0")"
    syncoid="/usr/sbin/syncoid"
    sanoid="/usr/sbin/sanoid"

    case "$prog" in
        update-sanoid)
            install
            check_config
            ;;
        syncoid)
            [ -x "$syncoid" ] && exec "$syncoid" "$@"
            ;;
        sanoid)
            [ -x "$sanoid" ] && exec "$sanoid" "$@"
            ;;
        sanoid-sync)
            sanoid_sync "$@"
            ;;
        *)
            err "$prog: Unknown command"
            exit 1
            ;;
    esac
}


#
# Install if needed
#
install()
{
    local ans

    if [ ! -x /usr/sbin/sanoid ]
    then
        echo
        echo -n "sanoid not installed. Do you want to install it [y/n] ? "
        if /usr/bin/tty -s
        then
            read ans
        else
            ans="n"
        fi
        case "$ans" in
        [yY]*)
            echo "installing sanoid from https://github.com/jimsalterjrs/sanoid.git"
            echo "to /opt/sanoid"
            /usr/bin/apt install -y debhelper libcapture-tiny-perl libconfig-inifiles-perl pv lzop mbuffer build-essential git
            cd /opt || (echo "cd to /opt failed, exiting"; exit 1)
            [ -d sanoid/.git ] || /usr/bin/git clone https://github.com/jimsalterjrs/sanoid.git
            cd sanoid
            /usr/bin/git checkout $(git tag | grep "^v" | tail -n 1)
            [ -d debian ] || /usr/bin/ln -s packages/debian .
            [ -e "../sanoid_*_all.deb" ] || /usr/bin/dpkg-buildpackage -uc -us
            /usr/bin/apt install ../sanoid_*_all.deb
            /usr/bin/systemctl enable --now sanoid.timer
            ;;
        *)
            echo "exiting."
            exit 0
            ;;
        esac
    fi
}


header()
{
/usr/bin/cat - <<-END
#
# This config file is built from;
#
#    /etc/pve/local/sanoid.conf
#    /etc/pve/nodes/sanoid-templates.conf
#
# By:
#
#    $(/bin/readlink -f "$(/bin/dirname "$0")")/${prog}
#
# Please do not edit directly as changes will be lost. Instead edit the top two files
# and then run update-sanoid
#

END
}


#
# check to make sure all vols are listed in the config file
#
check_vols()
{
    local msg i name conf_list
    declare -A conf_list

    # a list of volumes in sanoid.conf file
    conf_list=()
    if [ -e /etc/pve/local/sanoid.conf ]
    then
        while read name
        do
            case "$name" in
                "["*"]")

                    # remove the sqare blackets
                    name=${name##*[}
                    name=${name%%]*}

                    # split the volume path to an array
                    IFS=/ read -a i <<< "$name"
                    conf_list["$name"]="${i[0]}" # an entry for this config
                    conf_list["${i[0]}"]="$name" # also create an entry for the pool name
                    ;;
            esac

        done <<< "$(</etc/pve/local/sanoid.conf)"
    fi

    # now process all zfs volumes
    echo
    echo "# auto generated start"
    echo
    while read name
    do
        #echo check $name

        # split the volume path to an array
        IFS=/ read -a i <<< "$name"

        if [ -n "${conf_list[$name]}" ]
        then
            msg="  exists, ignoring"

        elif [ "${#i[@]}" == 2 ]
        then
            shopt -s extglob
            case "${i[1]}" in
                ROOT)
                    echo "[$name]"
                    echo "    use_template = root"
                    echo "    recursive = yes"
                    echo
                    ;;
                data|data[0-9]*)
                    echo "# Ignore Data for now. Need to do the VM's individually"
                    echo "#[$name]"
                    echo "#    use_template = data"
                    echo "#    recursive = yes"
                    echo
                    ;;
                pvs|pvs[0-9]*)
                    echo "[$name]"
                    echo "    use_template = pvs"
                    echo "    recursive = yes"
                    echo
                    ;;
                subvol-*|vm-*)
                    msg="  ignore vm or ct disk"
                    ;;
                *)
                    echo "# No rule for ${i[1]}"
                    echo "#[$name]"
                    echo "#    use_template = unknown"
                    echo "#    recursive = yes"
                    echo
                    ;;

            esac
        else
            msg="  ignore"
        fi

        #echo "$msg"

    done <<< "$(/usr/sbin/zfs list -H -o name)"

    echo "# auto generated end"
    echo
}

# pvesm path local-zfs:vm-130-disk-0
# /dev/zvol/rpool/data/vm-130-disk-0
#
# pvesm list local-zfs --vmid 102
#
# pvesm status
#
# pvesh get /storage/local-zfs
#
# pvesh get /storage/local-zfs --output-format yaml | fgrep "type: zfspool"
# pvesh get /storage/local-zfs --output-format yaml | fgrep "storage:"
#
# pvesh get /nodes/host04/qemu/ --output-format yaml | fgrep -e status: -e vmid:
#
# pvesh get /nodes/host04/lxc/102/config --output-format yaml | grep -e 'mp[0-9]\+:' -e rootfs:
#
# get /nodes/host04/qemu/133/config --output-format yaml | grep -e 'scsi[0-9]\+:' -e 'sata[0-9]\+:' -e 'virtio[0-9]\+:' -e 'ide[0-9]\+:'
#
# check for locks
# pvesh get /nodes/host04/lxc/115/pending --output-format yaml | # look for key: lock and it's value on next line; disk
#

#
# check config file: /etc/sanoid/sanoid.conf
# use /etc/pvs/local/sanoid.conf if it exists
#
check_config()
{
    [ -d /etc/sanoid ] || mkdir /etc/sanoid

    [ -e /etc/sanoid/sanoid.conf ] && /usr/bin/mv /etc/sanoid/sanoid.conf /etc/sanoid/sanoid.conf.bak

    header > /etc/sanoid/sanoid.conf

    [ -e /etc/pve/local/sanoid.conf ] && /usr/bin/cat /etc/pve/local/sanoid.conf >> /etc/sanoid/sanoid.conf

    check_vols >> /etc/sanoid/sanoid.conf

    [ -e /etc/pve/nodes/sanoid-templates.conf ] && /usr/bin/cat /etc/pve/nodes/sanoid-templates.conf >> /etc/sanoid/sanoid.conf
}



sanoid_sync()
{
    local list=()
    local arg arg_a arg_b

    if [ $# == 0 ]
    then
        usage
    else
        # build up a list of vm's to process untill we get a destination
        # storage location.
        # All command line switches are ignored and passed on to be processed
        # by sync_vms
        for arg in "$@"
        do
            case "$arg" in
                +([0-9]))
                            list+=("$arg");;
                +(--help|-h))
                            usage;;
                -*)
                            list+=("$arg");; # just pass the switches through
                *":"*)
                            arg_a="${arg##*:}"
                            arg_b="${arg%%:*}"

                            # if arg_b is a node, then process the vm's
                            if [ -d "/etc/pve/nodes/$arg_b" ]
                            then
                                sync_vms "$arg_a" "$arg_b" "${list[@]}"
                                list=()

                            # arg_b not a node, so just add to list of vm's
                            else
                                list+=("$arg") # just pass the on too. It could be disk:storage
                            fi
                            ;;
                *)
                            err "$arg: unknown option" 
                            exit 1
                            ;;
            esac
        done
    fi

    # process all errors encountered
    for er in "${error_list[@]}"
    do
        err "$er"
    done
}

usage()
{
    echo "Usage: $(/bin/basename "$0") <vm id>|<vm single disk:to-pool> ... <to-node:to-storage>" > /dev/stderr
    echo "    sync/move 'vm by it's id or a single disk' to to-storage on to-node" > /dev/stderr
    echo > /dev/stderr
    echo "Where:" > /dev/stderr
    echo "    <vm id> is copied or moved to <to-node:to-storage>" > /dev/stderr
    echo "    <vm single disk:to-pool> is copied to an alternate storage pool on to-node" > /dev/stderr
    echo "    --move  - move vm after sync" > /dev/stderr
    echo "    --force - force the zfs storage sync (be carefull with this option)" > /dev/stderr
    echo > /dev/stderr
    exit 0
}

msg()
{
    if [ $# == 1 ]
    then
        echo > /dev/stderr
    else
        echo "$@" > /dev/stderr
    fi
}

info()
{
    msg "Info:" "$@"
}

err()
{
    msg "Error:" "$@"
    error_list+=("$@")
}


# Given a proxmox pool name, will return the zfs pool location
# 
#
get_pool()
{
    if [ -n "$1" ]
    then
        if [ -z "${cluster_pool[$1]}" ]
        then
            info "get pool $1"
            cluster_pool[$1]="$(set -- $(/usr/bin/pvesh get "/storage/$1" --output-format yaml | /usr/bin/fgrep pool:); echo $2)"
            info "got ${cluster_pool[$1]}"
        fi
    fi

    [ -n "${cluster_pool[$1]}" ]
    return 
}

# get info about a vm from pvesh
get_pvesh_info()
{
    local value="$(/usr/bin/pvesh get /nodes/${vm_node[$1]}/${vm_type[$1]}/$1/$2 --output-format yaml | /bin/fgrep "$3:")"
    local ev="$?"
    value="${value##*$3: }"
    echo "$value"
    return "$ev"
}


#
# get_vm_storage vm...
#
# given one or move vm's, compose a list of storage used by these vm's in vm_storage_list[]
#
# updates
#   vm_storage_from[disk]
#   vm_storage_to[disk]
#   vm_storage_disk[disk]
#
get_vm_storage()
{
    local vm disk pool

    vm_storage_list=()

    for vm in "$@"
    do
        while read dev disk
        do
            disk="${disk%%,*}"
            pool="${disk%%:*}"

            if [ "$pool" != "$disk" ]
            then
                disk="${disk##*:}"
                vm_storage_list+=("$disk")
                if [ -z "${vm_storage_from["$disk"]}" ]
                then
                    if ! get_pool "$pool"
                    then
                        err "unable to find storage location for $to_storage"
                    else
                        vm_storage_from["$disk"]="${cluster_pool["$pool"]}"
                        vm_storage_disk["$disk"]="$vm"
                    fi
                fi

                if [ -z "${vm_storage_to["$disk"]}" ]
                then
                    vm_storage_to["$disk"]="$to_pool"
                    vm_storage_disk["$disk"]="$vm"
                fi
            fi
        done <<< "$(/usr/bin/pvesh \
            get \
            /nodes/${vm_node[$vm]}/${vm_type[$vm]}/${vm}/config \
            --output-format yaml | \
                /bin/grep \
                -e 'scsi[0-9]\+:' \
                -e 'sata[0-9]\+:' \
                -e 'ide[0-9]\+:' \
                -e 'virtio[0-9]\+:' \
                -e 'mp[0-9]\+:' \
                -e 'rootfs:')"
    done
}

get_vm_info()
{
    local vm conf config

    for vm in "$@"
    do
        if [ -z "${vm_type[$vm]}" -a -n "$vm" ]
        then
            info "get vm info for $vm"
            while read config
            do
                if [ -n "$config" ]
                then
                    conf="${config##/etc/pve/}"
                    conf="${conf##nodes/}"
                    IFS="/" read node type conf <<< "$conf"

                    info process: $node $type $conf
                    if [ "$node" == local ]
                    then
                        info local
                        vm_local[$vm]="1"
                    else
                        [ "$type" == "qemu-server" ] && type="qemu"
                        vm_node[$vm]="$node"
                        vm_type[$vm]="$type"
                        vm_conf[$vm]="$config"
                        vm_status[$vm]="$(get_pvesh_info $vm status/current status)"
                        info "vm_status[$vm] = ${vm_status[$vm]}"
                    fi
                else
                    err "unable to find infor for vm $vm"
                fi

            done <<< "$(/bin/find /etc/pve/local/ /etc/pve/nodes/ -name "${vm}.conf")"
            echo
        fi
    done
}

#
# sync vms to storage on remote host
#
# arg1 is the destination in the form node:storage
#
sync_vms()
{
    local vm to_node to_storage

    to_storage="$1"
    to_node="$2"
    shift 2

    if [ -z "$to_storage" ]
    then
        err "unable to get to_storage from $to_storage:$to_node"

    elif ! get_pool "$to_storage"
    then
        err "unable to find storage location for $to_storage"
    else
        to_pool="${cluster_pool["$to_storage"]}"
        if [  -z "$to_node" -o "$to_node" == "$local_node" ]
        then
            to="${to_pool}${to_vol}"
        else
            to="root@${to_node}-m:${to_pool}${to_vol}"
        fi

        info "processing vms = $@"
        info "to_storage     = $to_storage"
        info "to_node        = $to_node"
        info

        opt_force=""
        opt_move=""
        for vm in "$@"
        do
            case "$vm" in
                --force|-f)     opt_force="--force-delete";;
                --move|-m)      opt_move="1";;
                --dry-run|-n)  opt_dry_run="1";;
                *)          sync_vm "$vm";;
            esac
        done
    fi
}

#
# sync_vm <vm id> <to storage>
#
sync_vm()
{
    if [ $# -lt 1 ]
    then
        err "Usage: sync_vm <vm id>"
    else
        local vm="$1"

        case "$vm" in
            -*) # an upprecessed option, we should not be here
                err "unknown arg: $vm"
                ;;
            +([0-9])) # a vm
                if [ -n "${vm_sync_status[$vm]}" ]
                then
                    info "vm $vm -> $to_storage:$to_node already processed with status ${vm_sync_status[$vm]}"
                else
                    vm_sync_status[$vm]="ok"

                    get_vm_info "$vm"

                    info "processing vm = $vm"
                    info "to_node       = $to_node"
                    info "to_storage    = $to_storage"
                    info "to_pool       = $to_pool"
                    info "to            = $to"
                    info "opt_move      = $opt_force"
                    info "opt_force     = $opt_move"
                    if [ -n "${vm_node[$vm]}" ]
                    then
                        info "--- node stuff ---"
                        info "vm_node       = ${vm_node[$vm]}"
                        info "vm_type       = ${vm_type[$vm]}"
                        info "vm_status     = ${vm_status[$vm]}"
                        info "vm_local      = ${vm_local[$vm]}"
                        info "vm_conf       = ${vm_conf[$vm]}"
                        info "sync status   = ${vm_sync_status[$vm]}"

                        info "--- disk stuff ---"
                        get_vm_storage "$vm"
                        for disk in "${vm_storage_list[@]}"
                        do
                            info \
                                sync1 $vm $disk \
                                from ${vm_node[$vm]}:${vm_storage_from[$disk]} \
                                to $to_node:${vm_storage_to[$disk]} 
                            sync_storage "$disk" "${vm_node[$vm]}" "${vm_storage_from[$disk]}/$disk" "$to_node" "$to_pool/$disk"
                            if [ $? != 0 ]
                            then
                                err "disk sync ${vm_node[$vm]}:${vm_storage_from[$disk]} failed"
                                vm_sync_status[$vm]="fail"
                            fi
                        done
                    else
                        vm_sync_status[$vm]="location of vm $vm not found, skipping."
                        err "${vm_sync_status[$vm]}"
                    fi
                fi
                ;;
            *":"*)  # a single vm disk
                info "processing a single disk = $vm"
                disk="${vm%%:*}"
                pool="${vm##*:}"

                if ! get_pool "$pool"
                then
                    err "unable to find pool $pool"
                else
                    vm="${vm#*-}"
                    vm="${vm%%-*}"

                    get_vm_info "$vm"

                    vm_storage_to["$disk"]="${cluster_pool["$pool"]}"
                    vm_storage_disk["$disk"]="$vm"

                    get_vm_storage "$vm"

                    info \
                        sync $vm $disk \
                        from ${vm_node[$vm]}:${cluster_pool["$pool"]} \
                        to $to_node:${vm_storage_to["$disk"]} 
                    sync_storage "$disk" "${vm_node[$vm]}" "${vm_storage_from[$disk]}/$disk" "$to_node" "${vm_storage_to["$disk"]}/$disk"
                    #sync_storage "$disk" "${vm_node[$vm]}" "${cluster_pool["$pool"]}/$disk" "$to_node" "$to_pool/$disk"
                    if [ $? != 0 ]
                    then
                        vm_sync_status["$vm"]="failed."
                        vm_sync_status["${vm}-${name}"]="failed: $from/$name"
                        err "sync ${vm_node[$vm]}:${cluster_pool["$pool"]} -> $to_node:$to_pool"
                    fi
                fi
                ;;
        esac
    fi
    info
}

#
# returns node address name, /etc/hosts has to have an entry as
# node-m, which is the address of the host migration network address
#
get_node_address()
{
    if [ ! -e /etc/pve/nodes/$1 ]
    then
        err "$1: node does not exist"
        return 1
    elif [ "$1" == "$local_node" ]
    then
        return 0
    else
        echo "root@$1-m:"
        return 0
    fi
}

#
# sync storage from_node from_storage to_node to_storage
#
sync_storage()
{
    local disk="$1"
    shift
    local from_n
    local from_storage
    local to_n
    local to_storage
    local ev="0"

    from_n="$(get_node_address "${vm_node[$vm]}")"
    [ $? == 0 ] || ev="$?"

    from_storage="${vm_storage_from[$disk]}/$disk"

    to_n="$(get_node_address "$to_node")"
    [ $? == 0 ] || ev="$?"

    to_storage="${vm_storage_to["$disk"]}/$disk"

    [ -z "${vm_sync_status["$disk"]}" ] && vm_sync_status["$disk"]="starting"

    if [ "${vm_sync_status["$disk"]}" == ok ]
    then
        info "disk $disk already synced, skipping."

    elif [ "$ev" != 0 ]
    then
        err "Unable to get node address"
        vm_sync_status["$disk"]="Unable to get node address"

    elif [ -n "$from_n" -a -n "$to_n" ]
    then
        err "Both $from_n and $to_n are remote. One has to be local."
        vm_sync_status["$disk"]="failed"
        ev="1"
    else
        if [ "$opt_dry_run" == "1" ]
        then
            echo "$syncoid" $do_force "${opts[@]}" --recursive "$from_n$from_storage" "$to_n$to_storage"
        else
            "$syncoid" $do_force "${opts[@]}" --recursive "$from_n$from_storage" "$to_n$to_storage"
        fi
        ev="$?"

        if [ "$ev" == 0 ]
        then
            vm_sync_status["$disk"]="ok"
        else
            vm_sync_status["$disk"]="failed"
            err "sync of disk $disk to $to_n$to_storage failed."
        fi
    fi

    return "$ev"
}

sync_vmx()
{
    local vm_conf=""
    declare -A vm_list
    do_force=""
    do_move=""

    for vm in "$@"
    do
        vm_list["$vm"]="ok"

        if [ -e "/etc/pve/local/qemu-server/${vm}.conf" -o -e /etc/pve/local/lxc/${vm}.conf ]
        then
            echo "vm = $vm - processing"
            for v in "/dev/$from/vm-$vm-"* "/$from/subvol-$vm-"*
            do
                if [ -e "$v" ]
                then
                    name="$(/bin/basename "$v")"
                    case "$name" in
                        *"-part"*) ;;
                        *)
                            case "$v" in
                                "/dev/"*)
                                    echo "  processing dev disk $name"
                                    echo "  /usr/sbin/syncoid ${opts[@]} --recursive" "$from/$name" "$to/$name"
                                    /usr/sbin/syncoid $do_force "${opts[@]}" --recursive "$from/$name" "$to/$name"
                                    if [ $? != 0 ]
                                    then
                                        vm_list["$vm"]="failed."
                                        vm_list["${vm}-${name}"]="failed: $from/$name"
                                    fi
                                    ;;
                                *)
                                    echo "  processing vol disk $name"
                                    echo "  /usr/sbin/syncoid ${opts[@]} --recursive" "$from/$name" "$to/$name"
                                    /usr/sbin/syncoid $do_force "${opts[@]}" --recursive "$from/$name" "$to/$name"
                                    if [ $? != 0 ]
                                    then
                                        vm_list["$vm"]="failed."
                                        vm_list["${vm}-${name}"]="failed: $from/$name"
                                    fi
                                    ;;
                            esac
                    esac
                fi
            done
            echo

        elif [ "$vm" == move ]
        then
            do_move="true"
            vm_list["$vm"]="true"
        elif [ "$vm" == force ]
        then
            do_force="--force-delete"
            vm_list["$vm"]="true"
        else
            echo "vm = $vm - not on this node, skipping."
            echo 
        fi
    done

    echo
    echo now issue the following commands if the VM disks were copied while the VM was off
    echo and you want to move it. But will also need to update the storage location if it
    echo has changed. a .bak file will be creted. Just delete it once you are happy
    echo

    for vm in "${!vm_list[@]}"
    do
        if [ "${vm_list["$vm"]}" == ok ]
        then
            if [ -e /etc/pve/local/qemu-server/${vm}.conf ]
            then
                echo /bin/mv /etc/pve/local/qemu-server/${vm}.conf /etc/pve/nodes/${to_node}/qemu-server/${vm}.conf
                echo /usr/bin/perl -i.bak -p -e "s/$from_storage:/$to_storage:/g" /etc/pve/nodes/${to_node}/qemu-server/${vm}.conf
                echo /bin/rm /etc/pve/nodes/${to_node}/qemu-server/${vm}.conf.bak
                if [ "$do_move" == true ]
                then
                    /bin/mv /etc/pve/local/qemu-server/${vm}.conf /etc/pve/nodes/${to_node}/qemu-server/${vm}.conf
                    ev="$?"
                    /usr/bin/perl -i.bak -p -e "s/$from_storage:/$to_storage:/g" /etc/pve/nodes/${to_node}/qemu-server/${vm}.conf
                    ev+="$?"
                    echo "move: completed: ev=$ev"
                fi

            elif [ -e /etc/pve/local/lxc/${vm}.conf ]
            then
                echo /bin/mv /etc/pve/local/lxc/${vm}.conf /etc/pve/nodes/${to_node}/lxc/${vm}.conf
                echo /usr/bin/perl -i.bak -p -e "s/$from_storage:/$to_storage:/g" /etc/pve/nodes/${to_node}/lxc/${vm}.conf
                echo /bin/rm /etc/pve/nodes/${to_node}/lxc/${vm}.conf.bak
                if [ "$do_move" == true ]
                then
                    /bin/mv /etc/pve/local/lxc/${vm}.conf /etc/pve/nodes/${to_node}/lxc/${vm}.conf
                    ev="$?"
                    /usr/bin/perl -i.bak -p -e "s/$from_storage:/$to_storage:/g" /etc/pve/nodes/${to_node}/lxc/${vm}.conf
                    ev+="$?"
                    echo "move: completed: ev=$ev"
                fi
            fi
        else
            echo "$vm = ${vm_list["$vm"]}"
        fi
        echo
    done
}

main "$@"

